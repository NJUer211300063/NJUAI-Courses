# 第一章 绪论
基本术语 NFL定理 奥卡姆剃刀原则
# 第二章 模型评估与选择
- 经验误差、泛化误差
- 评估方法(如何获得测试结果)
    - 留出法、交叉验证法、自助法
    - 训练集、验证集、测试集
- 性能度量(如何评估优劣)
    - 错误率、精度
    - 查准率(P)、查全率(R)、PR图、F1度量、F_beta度量、宏\微查全率查准率
    - TPR、FPR、ROC曲线、AUC
    - 非均等代价(二分类代价矩阵、代价敏感错误率)
- 比较检验(如何判断实质差别) 
    - 平均序值、Friedman检验图 
- 偏差、方差分解
# 第三章 线性模型
- 线性回归
- 广义线性模型
    - 对数几率回归(分类)
- 线性判别分析(LDA)
    - 投影、协方差矩阵、类内散度矩阵、类间散度矩阵、全局散度矩阵
    - 优化目标(二分类、多分类)
- 多分类学习
    - 直接从二分类学习方法推广至多分类
    - 基于一些基本策略，使用二分类学习器解决多分类问题
        - OvO、OvM、MvM(EOOC输出纠错码)
- 类别不平衡
    - 欠采样、过采样、阈值移动        
# 第四章 决策树
- 基本流程(伪代码)
- 最优划分选择
    - 信息增益
    - 信息增益率
    - 基尼指数
- 剪枝处理(防止过拟合)
    - 预剪枝和后剪枝
- 连续值处理
    - 二分法
    - 需要注意，与离散属性不同，若当前节点划分属性为连续属性，该属性还可作为其后代节点的划分属性.
- 缺失值处理
    - 如何进行划分属性选择：仅通过无缺失值的样本来判断划分属性的优劣，为每个样本赋予一个权重，然后在生成决策树的过程中动态调整权重，对各种划分指标进行推广
    - 如何进行划分：如果某一样本再某一属性上的值未知，则让其以不同的概率划入到不同的子节点中去(也就是在这步调整权重)
- 进阶：多变量决策树
    - 不是为每个非叶节点找一个最优划分属性，而是试图建立一个合适的线性分类器.



# 第五章 神经网络
- 神经元模型
    - 输出  $y=f(\sum_{i=1}^{n}\omega_i x_i - \theta)$ 

            $\omega_i$: 第i个神经元的权重； 
            $x_i$: 第i个神经元的输入；
            $\theta$: 阈值
            $f$: 激活函数，如阶跃函数、Sigmoid函数
- 多层前馈神经网络
    - 每层神经元与下一层完全互连，神经元之间不存在同层连接，也不存在跨层连接
    - 输入层、输出层、隐层是指除了输入层和输出层之外的其他层
- BP算法
    - 基于梯度下降策略
    - 标准BP算法：每次更新只针对单个样例
    - 累计BP算法：针对累计误差最小化
    - 防止过拟合：
        - 早停
            - 若训练集误差降低但验证集误差提高则停止，返回具有最小验证集误差的连接权和阈值.
            - 若训练误差连续a轮的变化小于b，则停止训练
        - 正则化: $E=\lambda \frac{1}{m}\sum_{k=1}^{m}E_k + (1- \lambda)\sum_{i}\omega_i^2$
- 防止陷入局部最小
    - 不同的初始参数、“模拟退火”，随机梯度下降  
- some tricks   
    - 预训练+微调、权共享、Dropout、ReLU、交叉熵 



# 第六章 支持向量机
- 间隔: $\gamma = \frac{2}{\mid \mid \omega \mid \mid}$
- SVM基本型(最大化间隔): 
$$
\begin{align}
\min_{\omega, b} \quad&\frac{1}{2}\mid\mid \omega\mid\mid^2 \nonumber\\
\text{s.t.} \quad& y_i(\omega^{T}x_i+b) \geqslant 1,\quad i=1,2,\cdots,m \nonumber
\end{align} 
$$
- 拉格朗日乘子法、对偶问题、KKT条件
- 核函数
    - 低维空间线性不可分、高维空间线性可分：如果原始空间是有限维的，那么一定存在一个高位特征空间使样本线性可分
    - $\kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\left\langle\phi\left(\boldsymbol{x}_{i}\right), \phi\left(\boldsymbol{x}_{j}\right)\right\rangle=\phi\left(\boldsymbol{x}_{i}\right)^{\mathrm{T}} \phi\left(\boldsymbol{x}_{j}\right)$
    - 定理：$\kappa$是核函数 iff 核矩阵$K$半正定
    - 常用核函数、核函数的组合得到新的核函数
- 解
$$
\begin{aligned} f(\boldsymbol{x}) & =\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b \\ & =\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}+b\end{aligned}
$$
$$
\begin{aligned} f(\boldsymbol{x}) & =\boldsymbol{w}^{\mathrm{T}} \phi(\boldsymbol{x})+b \\ & =\sum_{i=1}^{m} \alpha_{i} y_{i} \phi\left(\boldsymbol{x}_{i}\right)^{\mathrm{T}} \phi(\boldsymbol{x})+b \\ & =\sum_{i=1}^{m} \alpha_{i} y_{i} \kappa\left(\boldsymbol{x}, \boldsymbol{x}_{i}\right)+b\end{aligned}
$$
- 软间隔支持向量机
    - 允许某些样本不满足约束$y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right) \geqslant 1$, 对这些样本加一个"惩罚"项  
    - 优化目标: $\min _{\boldsymbol{w}, b} \frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{i=1}^{m} \max \left(0,1-y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right)\right)$
    - 引入松弛变量：
    $$
    \begin{align*}
        \min _{\boldsymbol{w}, b, \xi_{i}} \quad &\frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{i=1}^{m} \xi_{i} \\
        \text { s.t. } \quad & y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right) \geqslant 1-\xi_{i} \\ & \xi_{i} \geqslant 0, i=1,2, \ldots, m .
    \end{align*}
    $$
- 支持向量回归
    - $f(\boldsymbol{x})=\sum_{i=1}^{m}\left(\hat{\alpha}_{i}-\alpha_{i}\right) \boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}+b$
- 表示定理 



# 第七章 贝叶斯分类器
- 贝叶斯判定准则：为了最小化总体风险，只需在每个样本上最小化条件风险
- 贝叶斯最优分类器: 满足贝叶斯判断准则的分类器
- 极大似然估计
- 朴素贝叶斯分类器
    - 假设每个属性独立地对分类结果发生影响 
    - $P(c \mid \boldsymbol{x})=\frac{P(c) P(\boldsymbol{x} \mid c)}{P(\boldsymbol{x})}=\frac{P(c)}{P(\boldsymbol{x})} \prod_{i=1}^{d} P\left(x_{i} \mid c\right)$ 
    - 拉普拉斯修正
- 半朴素贝叶斯分类器
    - 适当考虑一部分属性之间的相互依赖关系
    - 独依赖：假设每个属性在类别之外最多只依赖于一个其他属性
        - SPODE, TAN, AODE 
- 贝叶斯网
- EM算法


# 第八章 集成学习
- 误差-分歧分解: $E=\bar{E}-\bar{A}$
    - 个体学习器要好而不同
- 基本方法
    - 序列化方法：个体学习器存在强依赖关系，必须串行生成
    - 并行化方法：个体学习器不存在强依赖关系，可同时生成  
- Boosting
    - 先从初始训练集训练出一个学习器，然后根据基学习器的表现对样本分布进行调整，使得先前基学习器分类错误的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基学习器.
    - 有效降低偏差，将泛化性能弱的学习器提升到很强的集成
- Bagging
    - 使用自助采样法采样多个训练集，然后用这些训练集训练不同的基学习器最后组合
    - 有效减低方差，对易受样本干扰的学习上效果明显
    - 随机森林：在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练中引入随机属性选择(核心).
- 结合策略
    - 对于回归问题，常采用平均法: 简单平均法、加权平均法
    - 对于分类问题，常采用投票法：绝对多数投票法、相对多数投票法、加权投票法  硬投票、软投票
    - 学习法：Stacking学习法
- 多样性度量
- 多样性增强
    - 数据样本扰动、输入属性扰动、输出表示扰动、算法参数扰动   



# 第九章 聚类

- 聚类任务:
    - 通过对无标记训练样本的学习来揭示数据的内在性质及规律，为进一步的数据分析提供基础
- 性能度量(簇内相似度高、簇间相似度低)
    - 外部指标
        - Jaccard系数、FM指数、Rand指数，这些指标的取值都在$[0, 1]$， 值越大越好
    - 内部指标
        - DBI、DI， DBI的值越小越好，DI则相反
- 距离计算
    - 有序属性
        - 闵可夫斯基距离、加权闵可夫斯基距离
    - 无序属性
        - VDM距离、加权VDM距离
    - 混合属性
        - 将闵可夫斯基距离和VDM距离结合
- 原型聚类
    - k-means算法、LVQ算法(采用原型向量刻画聚类结构)
    - 高斯混合聚类(采用概率模型来表达聚类原型)
- 密度聚类         
    - DBSCAN算法      
- 层次聚类
    - AGNES算法  
- 三种聚类方法的对比 




# 第十章 降维：主成分分析
- k近邻学习
- 线性降维：基于线性变换来进行降维的方法
- 主成分分析(PCA)
    - 最近重构性
    - 最大可分性
- 核化线性降维  